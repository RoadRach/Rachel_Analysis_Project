Preparation:
#allow user to see structure of program
brew install tree
#creating python virtual env
$ cd /scrapy_tutorials
$ python3 -m venv venv
$ source venv/bin/activate
#brew install scrapy


How to start a scrapy project:
scrapy startproject <project_name>

How to create a spider:
scrapy genspider <spiderName> <nameOfWebsite>

Ways to request data:
response.css('.absolute-link-wrapper > div:nth-of-type(2)::attr(content)').extract_first()
response.css('.product-inner > .absolute-link-wrapper > div:nth-of-type(2)::attr(content)').extract()

To output file from response:

scrapy crawl mooncakespider -O vitaminc-data.csv

--------------------------------------------------------
AIRFLOW

Setting up local airflow:

export AIRFLOW_HOME=export AIRFLOW_HOME="$(pwd)"

Google apache airflow and go to installing for PyPi, get the text file installation

airflow standalone

To find where is the port is being used:
lsof -i tcp:3000

To kill ports:
kill -9 $(lsof -ti:3000,3001)

To link local content to docker container:
Go to docker yaml file > under volumnes add a new volumne.  Exxample : - ${AIRFLOW_PROJ_DIR:-.}/scripts:/opt/airflow/scripts